# Protego AI Usage Flow - Complete Guide

**Last Updated:** 2026-01-16
**Version:** 2.0

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [AI Services Architecture](#ai-services-architecture)
3. [Complete AI Flow Diagram](#complete-ai-flow-diagram)
4. [Four AI Modes](#four-ai-modes)
   - [One-Time Audio Analysis](#1-one-time-audio-analysis)
   - [Real-Time Audio Monitoring](#2-real-time-audio-monitoring)
   - [Text Analysis & Chat](#3-text-analysis--chat)
   - [AI Safety Call](#4-ai-safety-call-new)
5. [Key Components](#key-components)
6. [Distress Detection Engine](#distress-detection-engine)
7. [Alert Flow](#alert-flow)
8. [API Reference](#api-reference)
9. [Configuration](#configuration)
10. [Rate Limiting](#rate-limiting)
11. [Test Mode](#test-mode)

---

## Overview

Protego uses **three AI services** working together to provide comprehensive safety monitoring:

| Service | Provider | Purpose | Model |
|---------|----------|---------|-------|
| **Whisper** | Chutes AI | Audio transcription | whisper-large-v3 |
| **MegaLLM** | MegaLLM.io | Text analysis & chat | claude-opus-4.5 |
| **Azure OpenAI Realtime** | Microsoft | Real-time audio monitoring & safety calls | gpt-4o-realtime-preview |

**Key Features:**
- Real-time distress detection from audio
- AI-powered safety analysis and recommendations
- Continuous audio monitoring with WebSocket
- Post-walk session summaries
- Location-based safety assessment
- Interactive safety assistant
- **ğŸ†• AI Safety Call** - Fake phone call with AI friend for deterrence

---

## AI Services Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERACTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  AUDIO INPUT   â”‚         â”‚  TEXT INPUT    â”‚
            â”‚  (Microphone)  â”‚         â”‚  (Chat/Voice)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
        â”‚           â”‚           â”‚              â”‚
        â–¼           â–¼           â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ONE-TIME  â”‚  â”‚REAL-TIME â”‚  â”‚  SAFETY  â”‚  â”‚   CHAT   â”‚
â”‚ANALYSIS  â”‚  â”‚MONITORINGâ”‚  â”‚   CALL   â”‚  â”‚ASSISTANT â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚              â”‚             â”‚
     â”‚             â”‚              â”‚             â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND AI SERVICES                            â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   WHISPER      â”‚  â”‚  AZURE OPENAI  â”‚  â”‚    MEGALLM      â”‚   â”‚
â”‚  â”‚   (Chutes)     â”‚  â”‚   REALTIME     â”‚  â”‚   (Claude-Opus-4.5)     â”‚   â”‚
â”‚  â”‚                â”‚  â”‚                â”‚  â”‚                 â”‚   â”‚
â”‚  â”‚ Transcribes    â”‚  â”‚ WebSocket      â”‚  â”‚ Analyzes text   â”‚   â”‚
â”‚  â”‚ audio to text  â”‚  â”‚ live audio +   â”‚  â”‚ Safety analysis â”‚   â”‚
â”‚  â”‚                â”‚  â”‚ conversation   â”‚  â”‚ Chat responses  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                   â”‚                     â”‚           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                       â”‚                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚  DISTRESS        â”‚                              â”‚
â”‚              â”‚  DETECTION       â”‚                              â”‚
â”‚              â”‚  ENGINE          â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                       â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   CONFIDENCE CHECK    â”‚
            â”‚   >= 0.8 threshold?   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                  â”‚           â”‚
                 YES          NO
                  â”‚           â”‚
                  â–¼           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   CREATE   â”‚  â”‚   LOG    â”‚
         â”‚   ALERT    â”‚  â”‚   ONLY   â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  START COUNTDOWN      â”‚
    â”‚  (5 seconds default)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚
    USER CANCELS   COUNTDOWN EXPIRES
         â”‚             â”‚
         â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CANCEL  â”‚  â”‚   TRIGGER    â”‚
    â”‚ ALERT   â”‚  â”‚ SMS TO TRUSTEDâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   CONTACTS   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete AI Flow Diagram

### High-Level Flow

```
User Action â†’ Audio/Text Input â†’ Backend Processing â†’ AI Analysis â†’
Distress Detection â†’ Confidence Check â†’ Alert Decision â†’
Countdown Timer â†’ SMS Notification
```

### Detailed Component Interactions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  AudioMonitorâ”‚  â”‚  Realtime    â”‚  â”‚  ChatAssistant â”‚        â”‚
â”‚  â”‚  Component   â”‚  â”‚  Hook        â”‚  â”‚  Component     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                 â”‚                   â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                   â”‚
          â”‚ HTTP POST       â”‚ WebSocket         â”‚ HTTP POST
          â”‚ /api/ai/        â”‚ wss://azure       â”‚ /api/ai/
          â”‚ analyze/audio   â”‚                   â”‚ chat
          â”‚                 â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BACKEND LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚            FastAPI Router Layer                       â”‚       â”‚
â”‚  â”‚  /api/ai/analyze/audio                               â”‚       â”‚
â”‚  â”‚  /api/ai/analyze/text                                â”‚       â”‚
â”‚  â”‚  /api/ai/chat                                        â”‚       â”‚
â”‚  â”‚  /api/ai/summary/session/{id}                       â”‚       â”‚
â”‚  â”‚  /api/ai/analyze/location                           â”‚       â”‚
â”‚  â”‚  /api/ai/realtime/config                            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                       â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚         AI Service (ai_service.py)                   â”‚       â”‚
â”‚  â”‚                                                      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚       â”‚
â”‚  â”‚  â”‚  Method Routing                            â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ transcribe_audio()                     â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ analyze_audio_for_distress()           â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ analyze_with_llm()                     â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ chat_safety_assistant()                â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ generate_safety_summary()              â”‚     â”‚       â”‚
â”‚  â”‚  â”‚  â€¢ analyze_location_safety()              â”‚     â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚              â”‚                                                   â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚      â”‚                â”‚              â”‚                â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚              â”‚                â”‚
       â”‚ HTTP POST      â”‚ HTTP POST    â”‚ WebSocket      â”‚
       â”‚                â”‚              â”‚ (Frontend)     â”‚
       â–¼                â–¼              â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  Whisper    â”‚  â”‚   MegaLLM    â”‚  â”‚ Azure OpenAI   â”‚ â”‚
â”‚  Chutes AI  â”‚  â”‚   Claude-Opus-4.5    â”‚  â”‚   Realtime     â”‚â—„â”˜
â”‚             â”‚  â”‚              â”‚  â”‚                â”‚
â”‚ Transcribe  â”‚  â”‚ Text analysisâ”‚  â”‚ Live audio +   â”‚
â”‚ audio       â”‚  â”‚ Chat         â”‚  â”‚ transcription  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Four AI Modes

### 1. One-Time Audio Analysis

**Endpoint:** `POST /api/ai/analyze/audio`

**Use Case:** User records 3-10 seconds of audio for distress analysis

#### Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Records     â”‚
â”‚ Audio Clip       â”‚
â”‚ (3-10 seconds)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend sends   â”‚
â”‚ audio file       â”‚
â”‚ (webm/wav/mp3)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: ai_service.analyze_audio_      â”‚
â”‚                for_distress()           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Whisper Transcription           â”‚
â”‚                                         â”‚
â”‚ â€¢ Send audio bytes to Chutes API        â”‚
â”‚ â€¢ Receive segments with timestamps      â”‚
â”‚                                         â”‚
â”‚ Example Response:                       â”‚
â”‚ [                                       â”‚
â”‚   {                                     â”‚
â”‚     "text": "help me please",          â”‚
â”‚     "start": 0.0,                      â”‚
â”‚     "end": 2.5                         â”‚
â”‚   }                                     â”‚
â”‚ ]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Keyword Detection (Rule-Based)  â”‚
â”‚                                         â”‚
â”‚ Combine all text segments:              â”‚
â”‚ "help me please stop"                   â”‚
â”‚                                         â”‚
â”‚ Check distress keywords:                â”‚
â”‚ âœ“ "help" (found)                       â”‚
â”‚ âœ“ "help me" (found)                    â”‚
â”‚ âœ“ "please" (found)                     â”‚
â”‚                                         â”‚
â”‚ Check scream indicators:                â”‚
â”‚ âœ— No scream detected                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Classification & Confidence     â”‚
â”‚                                         â”‚
â”‚ Logic:                                  â”‚
â”‚ â€¢ If scream detected:                   â”‚
â”‚   â†’ SCREAM (confidence: 0.9)           â”‚
â”‚                                         â”‚
â”‚ â€¢ Else if "help" keywords:              â”‚
â”‚   â†’ HELP_CALL (confidence: 0.95)       â”‚
â”‚                                         â”‚
â”‚ â€¢ Else if "crying" keywords:            â”‚
â”‚   â†’ CRYING (confidence: 0.7)           â”‚
â”‚                                         â”‚
â”‚ â€¢ Else if multiple keywords:            â”‚
â”‚   â†’ PANIC (confidence: 0.8)            â”‚
â”‚                                         â”‚
â”‚ Result: HELP_CALL (0.95)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Alert Decision                  â”‚
â”‚                                         â”‚
â”‚ if confidence >= 0.8:                   â”‚
â”‚   â€¢ Create Alert in database            â”‚
â”‚   â€¢ Set status = PENDING                â”‚
â”‚   â€¢ Store location, session_id          â”‚
â”‚   â€¢ Start 5-second countdown            â”‚
â”‚   â€¢ Return alert_id                     â”‚
â”‚ else:                                   â”‚
â”‚   â€¢ Return analysis only                â”‚
â”‚   â€¢ No alert created                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Code Flow

```python
# Step 1: Transcribe
segments = await ai_service.transcribe_audio(
    audio_data=audio_bytes,
    filename="audio.webm"
)
# Result: [WhisperSegment(text="help me please", start=0.0, end=2.5)]

# Step 2: Keyword Detection
full_text = " ".join(seg.text for seg in segments).lower()
# "help me please"

keywords_found = []
for keyword in DISTRESS_KEYWORDS:
    if keyword.lower() in full_text:
        keywords_found.append(keyword)
# ["help", "help me", "please"]

# Step 3: Classification
if "help" in keywords_found:
    distress_type = DistressType.HELP_CALL
    confidence = 0.95

# Step 4: Alert Creation
if confidence >= 0.8:
    alert = Alert(
        user_id=user.id,
        type=AlertType.VOICE_ACTIVATION,
        confidence=0.95,
        status=AlertStatus.PENDING
    )
    db.add(alert)
    db.commit()

    asyncio.create_task(alert_manager.start_alert_countdown(alert.id))
```

#### API Request

```bash
POST /api/ai/analyze/audio
Content-Type: multipart/form-data
Authorization: Bearer {token}

Fields:
  audio: [binary audio file]
  session_id: 123 (optional)
  location_lat: 40.7128 (optional)
  location_lng: -74.0060 (optional)
```

#### API Response

```json
{
  "transcription": "help me please someone",
  "distress_detected": true,
  "distress_type": "HELP_CALL",
  "confidence": 0.95,
  "keywords_found": ["help", "help me", "please"],
  "alert_triggered": true,
  "alert_id": 456
}
```

#### Rate Limit

```
50 requests per hour per IP address
```

---

### 2. Real-Time Audio Monitoring

**Technology:** WebSocket connection to Azure OpenAI Realtime API

**Use Case:** Continuous audio monitoring during walk sessions

#### Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Enables         â”‚
â”‚ Realtime Monitoring  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Get Config â”‚
â”‚ GET /api/ai/         â”‚
â”‚     realtime/config  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend Returns:                   â”‚
â”‚ {                                  â”‚
â”‚   "ws_url": "wss://...?api-key=..",â”‚
â”‚   "deployment": "gpt-4o-realtime", â”‚
â”‚   "instructions": "You are a..."   â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Connect    â”‚
â”‚ WebSocket            â”‚
â”‚                      â”‚
â”‚ const ws = new       â”‚
â”‚   WebSocket(ws_url)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Send Session Configuration         â”‚
â”‚                                    â”‚
â”‚ {                                  â”‚
â”‚   "type": "session.update",       â”‚
â”‚   "session": {                    â”‚
â”‚     "modalities": ["text","audio"],â”‚
â”‚     "input_audio_format": "pcm16",â”‚
â”‚     "turn_detection": {           â”‚
â”‚       "type": "server_vad",       â”‚
â”‚       "threshold": 0.5,           â”‚
â”‚       "silence_duration_ms": 500  â”‚
â”‚     }                             â”‚
â”‚   }                               â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Start Microphone     â”‚
â”‚                      â”‚
â”‚ â€¢ Sample rate: 24kHz â”‚
â”‚ â€¢ Format: PCM16      â”‚
â”‚ â€¢ Mono channel       â”‚
â”‚ â€¢ Noise suppression  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Capture Audio Loop                 â”‚
â”‚                                    â”‚
â”‚ Every 4096 samples (~170ms):       â”‚
â”‚                                    â”‚
â”‚ 1. Capture raw audio               â”‚
â”‚    Float32Array (24kHz)            â”‚
â”‚                                    â”‚
â”‚ 2. Convert to PCM16                â”‚
â”‚    Int16 array                     â”‚
â”‚                                    â”‚
â”‚ 3. Encode to Base64                â”‚
â”‚    String                          â”‚
â”‚                                    â”‚
â”‚ 4. Send via WebSocket              â”‚
â”‚    {                               â”‚
â”‚      "type": "input_audio_buffer   â”‚
â”‚              .append",             â”‚
â”‚      "audio": "base64_data"        â”‚
â”‚    }                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure OpenAI Processing            â”‚
â”‚                                    â”‚
â”‚ Server-side:                       â”‚
â”‚ â€¢ Voice Activity Detection (VAD)   â”‚
â”‚ â€¢ Automatic speech detection       â”‚
â”‚ â€¢ Real-time transcription          â”‚
â”‚ â€¢ AI analysis (GPT-4o-realtime)   â”‚
â”‚ â€¢ Structured JSON response         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebSocket Events Received          â”‚
â”‚                                    â”‚
â”‚ 1. "input_audio_buffer.           â”‚
â”‚     speech_started"                â”‚
â”‚    â†’ User started speaking         â”‚
â”‚                                    â”‚
â”‚ 2. "conversation.item.input_audio  â”‚
â”‚     _transcription.completed"     â”‚
â”‚    â†’ {                            â”‚
â”‚        "transcript": "help me"    â”‚
â”‚      }                            â”‚
â”‚                                    â”‚
â”‚ 3. "response.text.done"           â”‚
â”‚    â†’ {                            â”‚
â”‚        "distress_detected": true, â”‚
â”‚        "distress_type": "HELP_CALLâ”‚
â”‚        "confidence": 0.95,        â”‚
â”‚        "keywords": ["help"],      â”‚
â”‚        "action": "trigger_alert"  â”‚
â”‚      }                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend Callback    â”‚
â”‚ onDistressDetected() â”‚
â”‚                      â”‚
â”‚ â€¢ Show emergency UI  â”‚
â”‚ â€¢ Create alert via   â”‚
â”‚   POST /api/alerts/  â”‚
â”‚ â€¢ Start countdown    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Features

- **Instant Detection:** ~500ms latency from speech to analysis
- **Continuous Monitoring:** Always listening, no recording delay
- **AI-Powered:** Understands context, not just keywords
- **Server VAD:** Automatic speech detection on server side
- **Streaming Transcription:** Real-time text as user speaks

#### Code Implementation

**Frontend Hook: `useRealtimeAudio.ts`**

```typescript
// 1. Connect to WebSocket
const connect = async () => {
  const config = await aiAPI.getRealtimeConfig();
  const ws = new WebSocket(config.ws_url);

  ws.onopen = () => {
    // Configure session
    ws.send(JSON.stringify({
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        input_audio_format: 'pcm16',
        turn_detection: { type: 'server_vad' }
      }
    }));
  };

  ws.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.type === 'conversation.item.input_audio_transcription.completed') {
      console.log('Heard:', data.transcript);
      onTranscript?.(data.transcript);
    }

    if (data.type === 'response.text.done') {
      const analysis = JSON.parse(data.text);
      if (analysis.distress_detected) {
        onDistressDetected?.(analysis);
      }
    }
  };
};

// 2. Start listening
const startListening = async () => {
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: { sampleRate: 24000, channelCount: 1 }
  });

  const audioContext = new AudioContext({ sampleRate: 24000 });
  const processor = audioContext.createScriptProcessor(4096, 1, 1);

  processor.onaudioprocess = (e) => {
    const inputData = e.inputBuffer.getChannelData(0);
    const pcm16 = floatTo16BitPCM(inputData);
    const base64 = arrayBufferToBase64(pcm16);

    ws.send(JSON.stringify({
      type: 'input_audio_buffer.append',
      audio: base64
    }));
  };
};
```

**Backend Config Endpoint:**

```python
@router.get("/realtime/config")
async def get_realtime_config(current_user: User = Depends(get_current_user)):
    ws_url = (
        f"{settings.azure_openai_realtime_endpoint}"
        f"/openai/realtime?api-version=2024-10-01-preview"
        f"&deployment={settings.azure_openai_realtime_deployment}"
        f"&api-key={settings.azure_openai_realtime_api_key}"
    )

    return {
        "ws_url": ws_url,
        "deployment": settings.azure_openai_realtime_deployment,
        "instructions": """You are a safety monitoring AI for Protego.
            Listen for distress and respond in JSON format:
            {
              "distress_detected": boolean,
              "distress_type": "SCREAM" | "HELP_CALL" | "PANIC" | "CRYING" | "NONE",
              "confidence": float (0-1),
              "transcript": "what you heard",
              "keywords": ["distress", "keywords"],
              "action": "trigger_alert" | "monitor" | "none"
            }"""
    }
```

#### WebSocket Message Types

| Event Type | Direction | Description |
|-----------|-----------|-------------|
| `session.update` | Client â†’ Server | Configure session parameters |
| `session.created` | Server â†’ Client | Session initialized |
| `input_audio_buffer.append` | Client â†’ Server | Audio data chunk |
| `input_audio_buffer.speech_started` | Server â†’ Client | Speech detected |
| `input_audio_buffer.speech_stopped` | Server â†’ Client | Speech ended |
| `conversation.item.input_audio_transcription.completed` | Server â†’ Client | Transcription ready |
| `response.text.done` | Server â†’ Client | AI analysis complete |
| `error` | Server â†’ Client | Error occurred |

---

### 3. Text Analysis & Chat

**Use Case:** Analyze text input, chat with AI assistant, get safety tips

#### A) Quick Text Analysis

**Endpoint:** `POST /api/ai/analyze/text`

**Flow:**

```
User Types or Speaks
    â†“
Frontend converts to text
    â†“
POST /api/ai/analyze/text
{
  "text": "someone is following me",
  "context": "walking alone at night"
}
    â†“
Backend: ai_service.analyze_with_llm()
    â†“
MegaLLM API Call
    System: "You are a safety analysis AI..."
    User: "Analyze: 'someone is following me'"
    Temperature: 0.3 (deterministic)
    Model: claude-sonnet-4-5-20250929
    â†“
LLM Response (JSON)
{
  "is_emergency": true,
  "confidence": 0.85,
  "distress_type": "PANIC",
  "analysis": "User reports being followed - immediate concern",
  "recommended_action": "trigger_alert"
}
    â†“
Return to Frontend
```

**API Request:**

```bash
POST /api/ai/analyze/text
Content-Type: application/json
Authorization: Bearer {token}

{
  "text": "someone is following me",
  "context": "walking home from subway station"
}
```

**API Response:**

```json
{
  "is_emergency": true,
  "confidence": 0.85,
  "distress_type": "PANIC",
  "analysis": "User reports being followed - potential safety threat",
  "recommended_action": "trigger_alert"
}
```

**Rate Limit:** 100 requests/hour per IP

---

#### B) Chat Assistant

**Endpoint:** `POST /api/ai/chat`

**Flow:**

```
User: "What should I do if someone follows me?"
    â†“
POST /api/ai/chat
{
  "message": "What should I do if someone follows me?",
  "conversation_history": [
    {"role": "user", "content": "previous message"},
    {"role": "assistant", "content": "previous response"}
  ]
}
    â†“
Backend: ai_service.chat_safety_assistant()
    â†“
MegaLLM API Call
    System: "You are Protego's AI Safety Assistant..."
    Messages: [history + new message]
    Temperature: 0.7 (creative)
    Max tokens: 4000
    â†“
LLM Response (Natural Language)
"If you think someone is following you:

1. Cross the street and see if they follow
2. Head to a populated, well-lit area
3. Call a friend or 911 if you feel threatened
4. Use the SOS button in Protego immediately
5. Don't go home - they'll know where you live

Trust your instincts. It's better to be safe."
    â†“
Return to Frontend
```

**API Request:**

```bash
POST /api/ai/chat
Content-Type: application/json
Authorization: Bearer {token}

{
  "message": "What should I do if someone follows me?",
  "conversation_history": [
    {
      "role": "user",
      "content": "How does Protego work?"
    },
    {
      "role": "assistant",
      "content": "Protego monitors your safety..."
    }
  ]
}
```

**API Response:**

```json
{
  "response": "If you think someone is following you:\n\n1. Cross the street...",
  "timestamp": "2026-01-15T20:30:00Z"
}
```

**Rate Limit:** 30 requests/hour per IP

---

#### C) Safety Summary

**Endpoint:** `GET /api/ai/summary/session/{session_id}`

**Flow:**

```
Walk Session Ends
    â†“
Frontend: GET /api/ai/summary/session/123
    â†“
Backend fetches session data:
    - Duration: 45 minutes
    - Alerts: [
        {type: "SCREAM", confidence: 0.9, status: "cancelled"},
        {type: "VOICE_ACTIVATION", confidence: 0.85, status: "safe"}
      ]
    â†“
ai_service.generate_safety_summary()
    â†“
MegaLLM API Call
    Prompt: "Generate summary for 45-minute walk with 2 alerts..."
    Temperature: 0.5
    â†“
LLM Response
{
  "summary": "Completed 45-minute evening walk. Two alerts were
              triggered but both resolved safely. First alert at
              8:15 PM was a false positive (background noise).
              Second alert at 8:32 PM was user-initiated voice
              test. Overall safe journey.",
  "risk_level": "medium",
  "recommendations": [
    "Consider walking in groups during late hours",
    "Keep phone volume up to ensure alerts are heard",
    "Test voice activation in quieter environment"
  ],
  "alerts_analysis": "Both alerts were precautionary and
                      canceled by user within countdown period.
                      No actual danger detected."
}
```

**API Response:**

```json
{
  "summary": "Completed 45-minute evening walk...",
  "risk_level": "medium",
  "recommendations": [
    "Consider walking in groups during late hours",
    "Keep phone volume up"
  ],
  "alerts_analysis": "Both alerts were precautionary...",
  "session_duration_minutes": 45,
  "total_alerts": 2
}
```

---

#### D) Location Safety Analysis

**Endpoint:** `POST /api/ai/analyze/location`

**Flow:**

```
User Location: 40.7128, -74.0060
Time: 11:30 PM, Saturday
    â†“
POST /api/ai/analyze/location
{
  "latitude": 40.7128,
  "longitude": -74.0060,
  "timestamp": "2026-01-15T23:30:00Z",
  "context": "walking home from subway"
}
    â†“
Backend: Time Analysis (Heuristic)
    Hour: 23 (11 PM)
    is_night: true
    is_late_night: true

    Base safety score: 85
    Late night penalty: -25
    Weekend adjustment: -5

    Final score: 60
    â†“
MegaLLM API Call (if available)
    Prompt: "Analyze safety at coordinates..."
    Temperature: 0.4
    â†“
LLM Response
{
  "safety_score": 60,
  "status": "caution",
  "risk_level": "medium",
  "factors": [
    "Late night hours - reduced visibility and foot traffic",
    "Weekend night - be aware of surroundings",
    "Subway exit area - stay alert for groups"
  ],
  "recommendations": [
    "Stay on well-lit main streets",
    "Share live location with trusted contact",
    "Consider rideshare for last mile",
    "Keep phone easily accessible"
  ]
}
```

**API Request:**

```bash
POST /api/ai/analyze/location
Content-Type: application/json
Authorization: Bearer {token}

{
  "latitude": 40.7128,
  "longitude": -74.0060,
  "timestamp": "2026-01-15T23:30:00Z",
  "context": "walking home from subway"
}
```

**API Response:**

```json
{
  "safety_score": 60,
  "status": "caution",
  "risk_level": "medium",
  "factors": [
    "Late night hours - reduced visibility",
    "Weekend night - be aware of surroundings"
  ],
  "recommendations": [
    "Stay on well-lit routes",
    "Share live location with contacts",
    "Consider rideshare"
  ],
  "time_context": {
    "hour": 23,
    "is_night": true,
    "is_late_night": true,
    "day_of_week": "Saturday"
  },
  "analyzed_at": "2026-01-15T23:30:00Z"
}
```

**Rate Limit:** 100 requests/hour per IP

---

### 4. AI Safety Call ğŸ†•

**Technology:** WebSocket connection to Azure OpenAI Realtime API with bidirectional audio streaming

**Use Case:** User triggers fake phone call where AI acts as a concerned friend, creating deterrent effect while secretly detecting distress

#### Overview

The Safety Call feature allows users to start a realistic phone call with an AI that:
- Acts like a concerned friend checking on them
- Naturally mentions tracking their location
- Creates the impression someone is monitoring them (deterrent for threats)
- Detects distress keywords in user's speech
- Silently triggers alerts without revealing to potential attackers

#### Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Clicks          â”‚
â”‚ "Start Safety Call"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: POST       â”‚
â”‚ /api/safety-call/    â”‚
â”‚     start            â”‚
â”‚                      â”‚
â”‚ {                    â”‚
â”‚   location: {...}    â”‚
â”‚ }                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: SafetyCallManager         â”‚
â”‚                                    â”‚
â”‚ â€¢ Create session (UUID)            â”‚
â”‚ â€¢ Build AI prompt with context     â”‚
â”‚ â€¢ Initialize Azure Realtime        â”‚
â”‚ â€¢ Return WebSocket URL + config    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend Response:                  â”‚
â”‚ {                                  â”‚
â”‚   "session_id": "uuid...",        â”‚
â”‚   "connection": {                 â”‚
â”‚     "type": "websocket",          â”‚
â”‚     "url": "wss://azure...",      â”‚
â”‚     "protocol": "azure_realtime"  â”‚
â”‚   },                              â”‚
â”‚   "system_instructions": "You are â”‚
â”‚      a concerned friend calling..." â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Connect    â”‚
â”‚ WebSocket            â”‚
â”‚                      â”‚
â”‚ const ws = new       â”‚
â”‚   WebSocket(url)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Send Session Configuration         â”‚
â”‚                                    â”‚
â”‚ {                                  â”‚
â”‚   "type": "session.update",       â”‚
â”‚   "session": {                    â”‚
â”‚     "modalities": ["text","audio"],â”‚
â”‚     "voice": "alloy",             â”‚
â”‚     "input_audio_format": "pcm16",â”‚
â”‚     "output_audio_format": "pcm16"â”‚
â”‚     "instructions": "You are a... â”‚
â”‚        concerned friend calling to â”‚
â”‚        check on [name]'s safety.  â”‚
â”‚        Mention you're tracking    â”‚
â”‚        their location..."         â”‚
â”‚     "turn_detection": {           â”‚
â”‚       "type": "server_vad",       â”‚
â”‚       "threshold": 0.5,           â”‚
â”‚       "silence_duration_ms": 500  â”‚
â”‚     }                             â”‚
â”‚   }                               â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Start Microphone     â”‚
â”‚                      â”‚
â”‚ â€¢ Sample rate: 24kHz â”‚
â”‚ â€¢ Format: PCM16      â”‚
â”‚ â€¢ Mono channel       â”‚
â”‚ â€¢ Real-time stream   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bidirectional Audio Loop            â”‚
â”‚                                     â”‚
â”‚ USER â†’ AI:                          â”‚
â”‚ 1. Capture user audio (Float32)    â”‚
â”‚ 2. Convert to PCM16                 â”‚
â”‚ 3. Encode to Base64                 â”‚
â”‚ 4. Send via WebSocket:              â”‚
â”‚    {                                â”‚
â”‚      "type": "input_audio_buffer    â”‚
â”‚              .append",              â”‚
â”‚      "audio": "base64_data"         â”‚
â”‚    }                                â”‚
â”‚                                     â”‚
â”‚ AI â†’ USER:                          â”‚
â”‚ 1. Receive audio from Azure         â”‚
â”‚ 2. Decode Base64                    â”‚
â”‚ 3. Convert PCM16 to Float32         â”‚
â”‚ 4. Play through speaker             â”‚
â”‚                                     â”‚
â”‚ TRANSCRIPTS:                        â”‚
â”‚ â€¢ User speech transcribed           â”‚
â”‚ â€¢ AI responses transcribed          â”‚
â”‚ â€¢ Both sent to backend              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure OpenAI Processing            â”‚
â”‚                                    â”‚
â”‚ Server-side:                       â”‚
â”‚ â€¢ Voice Activity Detection (VAD)   â”‚
â”‚ â€¢ Real-time transcription          â”‚
â”‚ â€¢ AI conversation (GPT-4o-realtime)â”‚
â”‚ â€¢ Natural TTS output               â”‚
â”‚                                    â”‚
â”‚ AI Conversation Example:           â”‚
â”‚ AI: "Hey! Just checking in - I saw â”‚
â”‚      you're walking alone. I'm     â”‚
â”‚      following your location.      â”‚
â”‚      Everything okay?"             â”‚
â”‚                                    â”‚
â”‚ User: "help me please"             â”‚
â”‚                                    â”‚
â”‚ AI: "What's wrong? Talk to me -    â”‚
â”‚      I'm right here tracking you." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebSocket Events Received          â”‚
â”‚                                    â”‚
â”‚ 1. "conversation.item.input_audio  â”‚
â”‚     _transcription.completed"     â”‚
â”‚    â†’ {                            â”‚
â”‚        "transcript": "help me     â”‚
â”‚                      please"      â”‚
â”‚      }                            â”‚
â”‚    â†’ Frontend sends to backend:   â”‚
â”‚       POST /api/safety-call/      â”‚
â”‚            transcript             â”‚
â”‚                                    â”‚
â”‚ 2. "response.audio.delta"         â”‚
â”‚    â†’ AI speaking (audio chunks)   â”‚
â”‚    â†’ Play to user                 â”‚
â”‚                                    â”‚
â”‚ 3. "response.audio_transcript     â”‚
â”‚     .done"                        â”‚
â”‚    â†’ {                            â”‚
â”‚        "transcript": "What's wrongâ”‚
â”‚                       Talk to me" â”‚
â”‚      }                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: Process Transcript        â”‚
â”‚                                    â”‚
â”‚ POST /api/safety-call/transcript   â”‚
â”‚ {                                  â”‚
â”‚   "session_id": "uuid...",        â”‚
â”‚   "transcript": "help me please", â”‚
â”‚   "speaker": "user"               â”‚
â”‚ }                                  â”‚
â”‚                                    â”‚
â”‚ SafetyCallManager:                 â”‚
â”‚ â€¢ Add to session conversation      â”‚
â”‚ â€¢ Run DistressDetector.analyze()   â”‚
â”‚ â€¢ Check keywords: ["help", "help  â”‚
â”‚   me", "please"]                   â”‚
â”‚ â€¢ Confidence: 0.95 (HIGH)          â”‚
â”‚ â€¢ Trigger alert: YES               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silent Alert Triggering            â”‚
â”‚                                    â”‚
â”‚ Backend:                           â”‚
â”‚ â€¢ Create Alert (PENDING)           â”‚
â”‚ â€¢ Link to safety_call_session      â”‚
â”‚ â€¢ Send SMS to ALL trusted contacts:â”‚
â”‚                                    â”‚
â”‚   "ğŸš¨ EMERGENCY ALERT               â”‚
â”‚    from [Name]                     â”‚
â”‚                                    â”‚
â”‚    Safety call detected distress.  â”‚
â”‚                                    â”‚
â”‚    Location:                       â”‚
â”‚    [lat, lng]                      â”‚
â”‚    [Google Maps link]              â”‚
â”‚                                    â”‚
â”‚    Distress keywords: help me      â”‚
â”‚    Time: 11:30 PM                  â”‚
â”‚                                    â”‚
â”‚    Call them: [phone] or 911"      â”‚
â”‚                                    â”‚
â”‚ â€¢ Update session:                  â”‚
â”‚   distress_detected = true         â”‚
â”‚   alert_triggered = true           â”‚
â”‚   distress_keywords = [...]        â”‚
â”‚                                    â”‚
â”‚ Frontend:                          â”‚
â”‚ â€¢ NO indication shown to user      â”‚
â”‚ â€¢ Call continues normally          â”‚
â”‚ â€¢ AI acts naturally (no alarm)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Ends Call       â”‚
â”‚                      â”‚
â”‚ Frontend:            â”‚
â”‚ POST /api/safety-callâ”‚
â”‚      /end/{id}       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: End Session               â”‚
â”‚                                    â”‚
â”‚ SafetyCallManager:                 â”‚
â”‚ â€¢ Calculate duration               â”‚
â”‚ â€¢ Save conversation JSON           â”‚
â”‚ â€¢ Persist to database:             â”‚
â”‚   - Full transcript                â”‚
â”‚   - Distress keywords found        â”‚
â”‚   - Alert ID (if triggered)        â”‚
â”‚   - Start/end times                â”‚
â”‚   - Location                       â”‚
â”‚                                    â”‚
â”‚ Response:                          â”‚
â”‚ {                                  â”‚
â”‚   "session_id": "...",            â”‚
â”‚   "duration_seconds": 180,        â”‚
â”‚   "distress_detected": true,      â”‚
â”‚   "alert_triggered": true,        â”‚
â”‚   "alert_id": 789,                â”‚
â”‚   "conversation_summary": "Call   â”‚
â”‚      lasted 3 minutes. Distress   â”‚
â”‚      detected at 1:30. Alert sent â”‚
â”‚      to 3 contacts."              â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Features

- **Realistic Conversation:** Natural AI voice with conversational responses
- **Deterrent Effect:** Mentions tracking location to scare off potential threats
- **Silent Detection:** Distress keywords trigger alerts without user's knowledge
- **No Indication:** Call appears normal - attacker doesn't know alert was sent
- **Bidirectional Audio:** User hears AI, AI hears user (like real call)
- **Full Logging:** Complete transcript saved for evidence

#### API Endpoints

**1. Start Call**

```bash
POST /api/safety-call/start
Content-Type: application/json
Authorization: Bearer {token}

{
  "location": {
    "latitude": 40.7128,
    "longitude": -74.0060
  }
}
```

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "connection": {
    "type": "websocket",
    "url": "wss://your-resource.openai.azure.com/openai/realtime?...",
    "protocol": "azure_realtime"
  },
  "system_instructions": "You are a concerned friend calling to check on...",
  "created_at": "2026-01-16T20:30:00Z"
}
```

**2. Send Transcript (for distress detection)**

```bash
POST /api/safety-call/transcript
Content-Type: application/json
Authorization: Bearer {token}

{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "transcript": "help me please someone",
  "speaker": "user"
}
```

**Response:**
```json
{
  "status": "distress_detected",
  "distress_level": "high",
  "confidence": 0.95,
  "keywords_found": ["help", "help me", "please"],
  "alert_triggered": true,
  "alert_id": 789
}
```

**3. End Call**

```bash
POST /api/safety-call/end/{session_id}
Authorization: Bearer {token}
```

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "duration_seconds": 180,
  "distress_detected": true,
  "alert_triggered": true,
  "alert_id": 789,
  "conversation_summary": "Call lasted 3 minutes. Distress detected and alert sent to contacts.",
  "ended_at": "2026-01-16T20:33:00Z"
}
```

**4. Get Call History**

```bash
GET /api/safety-call/history
Authorization: Bearer {token}
```

**Response:**
```json
{
  "calls": [
    {
      "session_id": "...",
      "start_time": "2026-01-16T20:30:00Z",
      "duration_seconds": 180,
      "distress_detected": true,
      "alert_triggered": true,
      "location": { "latitude": 40.7128, "longitude": -74.0060 }
    }
  ]
}
```

**5. Get Stats**

```bash
GET /api/safety-call/stats
Authorization: Bearer {token}
```

**Response:**
```json
{
  "total_calls": 15,
  "total_duration_minutes": 45,
  "distress_detected_count": 2,
  "alerts_triggered_count": 1,
  "average_call_duration_seconds": 180,
  "last_call_at": "2026-01-16T20:30:00Z"
}
```

#### Distress Detection in Safety Calls

The Safety Call uses a **4-tier distress detection system**:

**Critical Keywords (Confidence: 0.95):**
- "help me", "someone help", "call 911", "call police"
- "i'm being attacked", "he's following me", "she's following me"

**High Priority (Confidence: 0.75-0.85):**
- "help", "emergency", "danger", "attack"
- "following me", "scared", "threatening"

**Medium Priority (Confidence: 0.50-0.65):**
- "uncomfortable", "nervous", "alone"
- "get away", "stop", "leave me alone"

**Low Priority (Confidence: 0.35):**
- "worried", "anxious", "dark", "afraid"

**Safe Phrases (Override - No Alert):**
- "I'm fine", "I'm okay", "just kidding"
- "everything's fine", "all good"

**Alert Threshold:** Confidence â‰¥ 0.75 triggers alert

#### AI Conversation Prompt

```
You are a concerned friend calling [Name] to check on their safety.

YOUR ROLE:
- Be warm, natural, and conversational like a REAL friend would be
- Keep responses SHORT (1-2 sentences max)
- Casually mention you're "checking their location" or "following their walk"
- If they seem distressed, subtly offer help WITHOUT alarming potential attackers

CONTEXT:
- User is currently at: [latitude, longitude]
- Time: [time of day]
- They are walking alone

CONVERSATION STYLE:
"Hey! Just wanted to check in - I saw you're out walking. I'm following your
location on the app. Everything going okay?"

IMPORTANT RULES:
- NEVER reveal this is an AI
- Stay in character as their concerned friend
- Be conversational and natural
- Don't be overly dramatic or suspicious
- If they say distress words, stay calm and supportive
- Keep mentioning you're "tracking them" for deterrent effect
```

#### Frontend Implementation

**React Hook: `useSafetyCall.ts`**

```typescript
export function useSafetyCall() {
  const [callActive, setCallActive] = useState(false)
  const [calling, setCalling] = useState(false)
  const [muted, setMuted] = useState(false)
  const [transcript, setTranscript] = useState<string[]>([])
  const [distressDetected, setDistressDetected] = useState(false)

  const wsRef = useRef<WebSocket | null>(null)
  const sessionIdRef = useRef<string | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)

  const startCall = async (location?: {...}) => {
    setCalling(true)

    // 1. Start session
    const response = await api.post('/safety-call/start', { location })
    const { session_id, connection, system_instructions } = response.data
    sessionIdRef.current = session_id

    // 2. Connect WebSocket
    const ws = new WebSocket(connection.url)
    wsRef.current = ws

    ws.onopen = async () => {
      // Configure session
      ws.send(JSON.stringify({
        type: 'session.update',
        session: {
          modalities: ['text', 'audio'],
          instructions: system_instructions,
          voice: 'alloy',
          input_audio_format: 'pcm16',
          output_audio_format: 'pcm16',
          turn_detection: { type: 'server_vad', threshold: 0.5 }
        }
      }))

      await startMicrophone()
      setCallActive(true)
      setCalling(false)
    }

    ws.onmessage = async (event) => {
      const data = JSON.parse(event.data)

      // User transcript
      if (data.type === 'conversation.item.input_audio_transcription.completed') {
        const userText = `You: ${data.transcript}`
        setTranscript(prev => [...prev, userText])

        // Send to backend for distress detection
        const result = await api.post('/safety-call/transcript', {
          session_id,
          transcript: data.transcript,
          speaker: 'user'
        })

        if (result.data.distress_detected) {
          setDistressDetected(true)
        }
      }

      // AI audio output
      if (data.type === 'response.audio.delta') {
        playAudio(data.delta)
      }

      // AI transcript
      if (data.type === 'response.audio_transcript.done') {
        setTranscript(prev => [...prev, `AI: ${data.transcript}`])
      }
    }
  }

  const startMicrophone = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: { sampleRate: 24000, channelCount: 1, echoCancellation: true }
    })

    const audioContext = new AudioContext({ sampleRate: 24000 })
    audioContextRef.current = audioContext

    const source = audioContext.createMediaStreamSource(stream)
    const processor = audioContext.createScriptProcessor(4096, 1, 1)

    processor.onaudioprocess = (e) => {
      if (muted) return

      const inputData = e.inputBuffer.getChannelData(0)
      const pcm16 = floatTo16BitPCM(inputData)
      const base64 = arrayBufferToBase64(pcm16)

      wsRef.current?.send(JSON.stringify({
        type: 'input_audio_buffer.append',
        audio: base64
      }))
    }

    source.connect(processor)
    processor.connect(audioContext.destination)
  }

  const endCall = async () => {
    wsRef.current?.close()
    audioContextRef.current?.close()

    if (sessionIdRef.current) {
      await api.post(`/safety-call/end/${sessionIdRef.current}`)
    }

    setCallActive(false)
    setTranscript([])
    setDistressDetected(false)
  }

  const toggleMute = () => setMuted(!muted)

  return {
    callActive,
    calling,
    muted,
    transcript,
    distressDetected,
    startCall,
    endCall,
    toggleMute
  }
}
```

#### Cost Analysis

**Azure OpenAI Realtime Pricing (as of Jan 2026):**

| Component | Rate | Per 5-min Call |
|-----------|------|----------------|
| Audio Input (Streaming) | ~$0.01/min | $0.05 |
| Audio Output (TTS) | ~$0.02/min | $0.10 |
| Text Tokens (GPT-4o) | ~$0.01/1K tokens | $0.01 |
| **Total** | | **~$0.16** |

**Monthly Estimates:**
- 100 calls/month: ~$16
- 500 calls/month: ~$80
- 1,000 calls/month: ~$160

**Cost Optimization Tips:**
- Use shorter system prompts (fewer tokens)
- Implement call duration limits (e.g., max 10 minutes)
- Cache common AI responses
- Monitor usage and set budget alerts

#### Security & Privacy

- **End-to-End Security:** WebSocket connections use TLS (wss://)
- **Authentication:** JWT required for all endpoints
- **Session Isolation:** Each call gets unique session ID
- **Data Retention:** Transcripts stored encrypted in database
- **Silent Alerts:** No UI indication prevents attacker awareness
- **Location Privacy:** GPS coordinates never sent to Azure (only backend)

#### Testing Checklist

- [ ] Start call and verify WebSocket connection
- [ ] Test microphone audio streaming
- [ ] Verify AI responds naturally in conversation
- [ ] Say distress keyword ("help") and check backend logs
- [ ] Confirm alert created (check database)
- [ ] Verify SMS sent to trusted contacts
- [ ] Test mute functionality
- [ ] End call and verify session saved
- [ ] Check call history endpoint
- [ ] Review conversation transcript in database
- [ ] Test with multiple distress keywords
- [ ] Test safe phrases ("I'm fine") - should NOT trigger alert
- [ ] Verify no UI indication when distress detected

---

## Key Components

### 1. AI Service Class

**Location:** `backend/services/ai_service.py`

```python
class AIService:
    """
    AI Service for audio analysis and safety intelligence.
    Integrates Whisper, MegaLLM, and Azure OpenAI Realtime.
    """

    def __init__(self):
        self.whisper_endpoint = settings.whisper_endpoint
        self.whisper_api_key = settings.whisper_api_key
        self.megallm_endpoint = settings.megallm_endpoint
        self.megallm_api_key = settings.megallm_api_key
        self.megallm_model = settings.megallm_model
        self.test_mode = settings.test_mode

    # Core Methods
    async def transcribe_audio(
        audio_data: bytes,
        filename: str
    ) -> List[WhisperSegment]

    async def analyze_audio_for_distress(
        audio_data: bytes
    ) -> AudioAnalysisResult

    async def analyze_with_llm(
        transcription: str,
        context: str
    ) -> Dict[str, Any]

    async def generate_safety_summary(
        user_name: str,
        session_duration_minutes: int,
        alerts: List[Dict]
    ) -> SafetySummary

    async def chat_safety_assistant(
        message: str,
        conversation_history: List[Dict]
    ) -> str

    async def analyze_location_safety(
        latitude: float,
        longitude: float,
        timestamp: str,
        user_context: str
    ) -> Dict[str, Any]
```

### 2. Data Classes

```python
from dataclasses import dataclass
from enum import Enum

class DistressType(str, Enum):
    SCREAM = "SCREAM"
    HELP_CALL = "HELP_CALL"
    CRYING = "CRYING"
    PANIC = "PANIC"
    NONE = "NONE"

@dataclass
class WhisperSegment:
    text: str
    start: float
    end: float

@dataclass
class AudioAnalysisResult:
    transcription: str
    distress_detected: bool
    distress_type: DistressType
    confidence: float
    keywords_found: List[str]
    segments: List[WhisperSegment]

@dataclass
class SafetySummary:
    summary: str
    risk_level: str  # low, medium, high
    recommendations: List[str]
    alerts_analysis: str
```

---

## Distress Detection Engine

### Keyword Lists

```python
# Primary distress keywords
DISTRESS_KEYWORDS = [
    # Calls for help
    "help", "help me", "someone help", "please help",

    # Commands to stop
    "stop", "let me go", "leave me alone",

    # Negative responses
    "no", "don't", "please don't",

    # Emergency terms
    "emergency", "call 911", "police",
    "fire", "attack", "danger",

    # Fear and pain
    "hurt", "pain", "scared",
    "run", "get away", "save me"
]

# Scream and audio indicators
SCREAM_INDICATORS = [
    "scream", "screaming",
    "yell", "yelling",
    "shout", "shouting",
    "cry", "crying",
    "[scream]", "[screaming]",
    "[yelling]", "[inaudible]",
    "[noise]", "[loud noise]"
]
```

### Classification Algorithm

```python
def classify_distress(
    transcription: str,
    keywords_found: List[str],
    scream_detected: bool
) -> Tuple[DistressType, float]:
    """
    Classify distress type and calculate confidence.

    Priority Order:
    1. Scream indicators (highest confidence)
    2. Explicit help keywords
    3. Crying indicators
    4. Multiple distress keywords (panic)
    5. Single keyword (lower confidence)
    """

    if scream_detected:
        return DistressType.SCREAM, 0.9

    # Check for explicit help
    help_keywords = ["help", "help me", "someone help", "please help"]
    if any(kw in keywords_found for kw in help_keywords):
        return DistressType.HELP_CALL, 0.95

    # Check for crying
    crying_keywords = ["crying", "cry"]
    if any(kw in keywords_found for kw in crying_keywords):
        return DistressType.CRYING, 0.7

    # Multiple keywords = panic
    if len(keywords_found) >= 2:
        return DistressType.PANIC, 0.8

    # Single keyword = lower confidence panic
    if len(keywords_found) == 1:
        return DistressType.PANIC, 0.6

    # No distress
    return DistressType.NONE, 0.0
```

### Confidence Thresholds

| Distress Type | Confidence | Alert Triggered (â‰¥0.8) |
|--------------|-----------|----------------------|
| SCREAM | 0.9 | âœ… Yes |
| HELP_CALL | 0.95 | âœ… Yes |
| PANIC (multiple keywords) | 0.8 | âœ… Yes |
| CRYING | 0.7 | âŒ No (logged only) |
| PANIC (single keyword) | 0.6 | âŒ No (logged only) |
| NONE | 0.0 | âŒ No |

**Default Threshold:** `ALERT_CONFIDENCE_THRESHOLD = 0.8`

---

## Alert Flow

### Complete Alert Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ALERT LIFECYCLE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Detection
   â†“
   Audio/Text analyzed â†’ Distress detected â†’ Confidence â‰¥ 0.8

2. Alert Creation
   â†“
   Create Alert record in database
   {
     user_id: 123,
     type: HELP_CALL,
     confidence: 0.95,
     status: PENDING,
     location_lat: 40.7128,
     location_lng: -74.0060,
     countdown_started_at: 2026-01-15T20:30:00Z,
     countdown_expires_at: 2026-01-15T20:30:05Z
   }

3. Countdown Start
   â†“
   Background task: alert_manager.start_alert_countdown(alert_id)

   Timer: 5 seconds (default)

   Frontend shows:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  âš ï¸  EMERGENCY DETECTED  â”‚
   â”‚                          â”‚
   â”‚  Triggering in 5s...     â”‚
   â”‚                          â”‚
   â”‚  [  CANCEL ALERT  ]      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. User Decision
   â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  User Cancels   â”‚ Countdown Expiresâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                 â”‚
            â–¼                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Update Alert   â”‚  â”‚ Trigger SMS  â”‚
   â”‚ status =       â”‚  â”‚              â”‚
   â”‚ CANCELLED      â”‚  â”‚ status =     â”‚
   â”‚                â”‚  â”‚ TRIGGERED    â”‚
   â”‚ cancelled_at = â”‚  â”‚              â”‚
   â”‚ timestamp      â”‚  â”‚ triggered_at=â”‚
   â”‚                â”‚  â”‚ timestamp    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Send SMS to ALL  â”‚
                    â”‚ Trusted Contacts â”‚
                    â”‚                  â”‚
                    â”‚ "EMERGENCY ALERT â”‚
                    â”‚ from {name}      â”‚
                    â”‚                  â”‚
                    â”‚ Location:        â”‚
                    â”‚ {coordinates}    â”‚
                    â”‚ {google_maps}    â”‚
                    â”‚                  â”‚
                    â”‚ Type: {type}     â”‚
                    â”‚ Time: {time}     â”‚
                    â”‚                  â”‚
                    â”‚ Call: {phone}    â”‚
                    â”‚ or 911"          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Alert Manager Service

**Location:** `backend/services/alert_manager.py`

```python
class AlertManager:
    """Manages alert countdowns and triggering."""

    def __init__(self):
        self.pending_alerts: Dict[int, asyncio.Task] = {}

    async def start_alert_countdown(self, alert_id: int) -> bool:
        """
        Start countdown timer for alert.
        Persists countdown state to database.
        """
        db = SessionLocal()
        try:
            alert = db.query(Alert).filter(Alert.id == alert_id).first()
            if not alert:
                return False

            # Persist countdown timestamps
            now = datetime.utcnow()
            expires_at = now + timedelta(seconds=settings.alert_countdown_seconds)

            alert.countdown_started_at = now
            alert.countdown_expires_at = expires_at
            db.commit()

            # Create countdown task
            task = asyncio.create_task(self._countdown_and_trigger(alert_id))
            self.pending_alerts[alert_id] = task

            return True
        finally:
            db.close()

    async def _countdown_and_trigger(self, alert_id: int):
        """Wait for countdown and trigger if not cancelled."""
        try:
            await asyncio.sleep(settings.alert_countdown_seconds)

            # Check if alert still pending
            db = SessionLocal()
            try:
                alert = db.query(Alert).filter(Alert.id == alert_id).first()

                if alert and alert.status == AlertStatus.PENDING:
                    await self._trigger_alert(alert_id, db)
            finally:
                db.close()
        finally:
            # Cleanup
            self.pending_alerts.pop(alert_id, None)

    async def _trigger_alert(self, alert_id: int, db: Session):
        """Trigger alert and send SMS to trusted contacts."""
        alert = db.query(Alert).filter(Alert.id == alert_id).first()
        if not alert:
            return

        # Update alert status
        alert.status = AlertStatus.TRIGGERED
        alert.triggered_at = datetime.utcnow()
        db.commit()

        # Send SMS to trusted contacts
        user = alert.user
        contacts = user.trusted_contact_list  # New TrustedContact model

        if not contacts and user.trusted_contacts:
            # Fallback to legacy JSON field
            contacts_data = user.trusted_contacts
        else:
            contacts_data = [
                {"phone": c.phone, "name": c.name}
                for c in contacts if c.is_active
            ]

        for contact in contacts_data:
            send_emergency_sms(
                to_phone=contact["phone"],
                user=user,
                alert=alert
            )

    async def cancel_alert(self, alert_id: int) -> bool:
        """Cancel pending alert."""
        # Cancel background task
        task = self.pending_alerts.pop(alert_id, None)
        if task:
            task.cancel()

        # Update database
        db = SessionLocal()
        try:
            alert = db.query(Alert).filter(Alert.id == alert_id).first()
            if alert and alert.status == AlertStatus.PENDING:
                alert.status = AlertStatus.CANCELLED
                alert.cancelled_at = datetime.utcnow()
                db.commit()
                return True
        finally:
            db.close()

        return False

    async def recover_pending_alerts(self) -> int:
        """
        Recover pending alerts from database after server restart.
        Called on startup.
        """
        db = SessionLocal()
        try:
            now = datetime.utcnow()

            # Find alerts with unexpired countdowns
            pending = db.query(Alert).filter(
                Alert.status == AlertStatus.PENDING,
                Alert.countdown_expires_at.isnot(None),
                Alert.countdown_expires_at > now
            ).all()

            # Find expired alerts that need triggering
            expired = db.query(Alert).filter(
                Alert.status == AlertStatus.PENDING,
                Alert.countdown_expires_at.isnot(None),
                Alert.countdown_expires_at <= now
            ).all()

            recovered = 0

            # Re-schedule pending alerts
            for alert in pending:
                time_remaining = (alert.countdown_expires_at - now).total_seconds()
                if time_remaining > 0:
                    task = asyncio.create_task(
                        self._countdown_with_remaining_time(alert.id, time_remaining)
                    )
                    self.pending_alerts[alert.id] = task
                    recovered += 1

            # Trigger expired alerts immediately
            for alert in expired:
                await self._trigger_alert(alert.id, db)
                recovered += 1

            return recovered
        finally:
            db.close()

# Global instance
alert_manager = AlertManager()
```

### Alert Recovery on Restart

```python
# In main.py startup
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("ğŸš€ Starting Protego Backend...")

    # Recover pending alerts
    recovered = await alert_manager.recover_pending_alerts()
    logger.info(f"ğŸ”„ Recovered {recovered} pending alerts")

    yield

    # Shutdown
    logger.info("ğŸ‘‹ Shutting down...")
```

---

## API Reference

### AI Endpoints Summary

| Endpoint | Method | Purpose | Rate Limit |
|----------|--------|---------|-----------|
| `/api/ai/analyze/audio` | POST | One-time audio analysis | 50/hour |
| `/api/ai/analyze/text` | POST | Quick text analysis | 100/hour |
| `/api/ai/chat` | POST | Chat with AI assistant | 30/hour |
| `/api/ai/summary/session/{id}` | GET | Session safety summary | - |
| `/api/ai/summary/latest` | GET | Latest session summary | - |
| `/api/ai/analyze/location` | POST | Location safety analysis | 100/hour |
| `/api/ai/tips` | GET | Get safety tips | - |
| `/api/ai/status` | GET | AI service status | - |
| `/api/ai/realtime/config` | GET | WebSocket config | - |
| **`/api/safety-call/start`** ğŸ†• | **POST** | **Start AI safety call** | **-** |
| **`/api/safety-call/transcript`** ğŸ†• | **POST** | **Process call transcript** | **-** |
| **`/api/safety-call/end/{id}`** ğŸ†• | **POST** | **End safety call** | **-** |
| **`/api/safety-call/history`** ğŸ†• | **GET** | **Get call history** | **-** |
| **`/api/safety-call/stats`** ğŸ†• | **GET** | **Get call statistics** | **-** |
| **`/api/safety-call/active`** ğŸ†• | **GET** | **Get active call count** | **-** |

### Authentication

All endpoints require authentication via:

**Method 1: httpOnly Cookie (Recommended)**
```bash
# Cookie set automatically by backend on login
# Sent automatically by browser
```

**Method 2: Bearer Token (Backward Compatible)**
```bash
Authorization: Bearer {access_token}
```

### Common Request/Response Schemas

#### AudioAnalysisResponse
```typescript
{
  transcription: string
  distress_detected: boolean
  distress_type: "SCREAM" | "HELP_CALL" | "CRYING" | "PANIC" | "NONE"
  confidence: number  // 0.0 - 1.0
  keywords_found: string[]
  alert_triggered: boolean
  alert_id: number | null
}
```

#### QuickAnalysisResponse
```typescript
{
  is_emergency: boolean
  confidence: number
  distress_type: string
  analysis: string
  recommended_action: "trigger_alert" | "monitor" | "none"
}
```

#### ChatResponse
```typescript
{
  response: string
  timestamp: string  // ISO 8601
}
```

#### SafetySummaryResponse
```typescript
{
  summary: string
  risk_level: "low" | "medium" | "high"
  recommendations: string[]
  alerts_analysis: string
  session_duration_minutes: number
  total_alerts: number
}
```

#### LocationSafetyResponse
```typescript
{
  safety_score: number  // 0-100
  status: "safe" | "caution" | "alert"
  risk_level: "low" | "medium" | "high"
  factors: string[]
  recommendations: string[]
  time_context: {
    hour: number
    is_night: boolean
    is_late_night: boolean
    day_of_week: string
  }
  analyzed_at: string  // ISO 8601
}
```

---

## Configuration

### Environment Variables

**Location:** `backend/.env`

```bash
# ===========================================
# AI Services Configuration
# ===========================================

# Whisper API (Chutes AI) - Audio transcription
WHISPER_ENDPOINT=https://chutes-whisper-large-v3.chutes.ai/transcribe
WHISPER_API_KEY=your_chutes_api_key_here

# MegaLLM API - Text analysis and chat
MEGALLM_ENDPOINT=https://ai.megallm.io/v1/chat/completions
MEGALLM_API_KEY=your_megallm_api_key_here
MEGALLM_MODEL=claude-sonnet-4-5-20250929

# Azure OpenAI Realtime - Real-time audio monitoring & safety calls
AZURE_OPENAI_REALTIME_ENDPOINT=wss://your-resource.openai.azure.com
AZURE_OPENAI_REALTIME_API_KEY=your_azure_api_key_here
AZURE_OPENAI_REALTIME_DEPLOYMENT=gpt-4o-realtime-preview

# ===========================================
# Safety Call Configuration ğŸ†•
# ===========================================

# Enable/disable safety call feature
SAFETY_CALL_ENABLED=true

# Maximum call duration in minutes (to prevent runaway costs)
SAFETY_CALL_MAX_DURATION_MINUTES=10

# Distress detection confidence threshold for safety calls
SAFETY_CALL_ALERT_THRESHOLD=0.75

# ===========================================
# Alert Configuration
# ===========================================

# Confidence threshold (0.0-1.0) for triggering alerts
ALERT_CONFIDENCE_THRESHOLD=0.8

# Countdown seconds before alert is triggered
ALERT_COUNTDOWN_SECONDS=5

# ===========================================
# Testing
# ===========================================

# Test mode prevents real SMS and simulates AI responses
TEST_MODE=false  # IMPORTANT: Set to false in production!
```

### Settings Class

**Location:** `backend/config.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # AI Services
    whisper_endpoint: str
    whisper_api_key: str
    megallm_endpoint: str
    megallm_api_key: str
    megallm_model: str = "claude-sonnet-4-5-20250929"
    azure_openai_realtime_endpoint: str = ""
    azure_openai_realtime_api_key: str = ""
    azure_openai_realtime_deployment: str = "gpt-4o-realtime-preview"

    # Alert Configuration
    alert_confidence_threshold: float = 0.8
    alert_countdown_seconds: int = 5

    # Testing
    test_mode: bool = False

    model_config = SettingsConfigDict(
        env_file=".env",
        case_sensitive=False
    )

settings = Settings()
```

---

## Rate Limiting

All AI endpoints are rate-limited using `slowapi` to prevent abuse.

### Rate Limit Configuration

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
```

### Endpoint Limits

| Endpoint | Limit | Reason |
|----------|-------|--------|
| `/api/ai/analyze/audio` | 50/hour | CPU-intensive transcription |
| `/api/ai/analyze/text` | 100/hour | Moderate LLM usage |
| `/api/ai/chat` | 30/hour | Conversational, prevent spam |
| `/api/ai/analyze/location` | 100/hour | Quick heuristic + LLM |

### Rate Limit Response

When rate limit is exceeded:

```json
{
  "error": "Rate limit exceeded: 50 per 1 hour"
}
```

**HTTP Status:** `429 Too Many Requests`

### Bypass Rate Limiting

For development or testing:

```python
# In main.py, comment out rate limiter
# app.state.limiter = limiter
```

---

## Test Mode

### Overview

Test mode allows development without consuming AI API credits or sending real SMS alerts.

### Enabling Test Mode

```bash
# In .env
TEST_MODE=true
```

### Test Mode Behavior

#### Whisper Transcription
```python
if test_mode:
    return [WhisperSegment(
        text="[Test transcription - AI service in test mode]",
        start=0.0,
        end=1.0
    )]
```

#### MegaLLM Analysis
```python
if test_mode:
    return {
        "is_emergency": False,
        "confidence": 0.5,
        "analysis": "Test mode - no actual analysis performed",
        "recommended_action": "none"
    }
```

#### Chat Assistant
```python
if test_mode:
    return (
        "I'm Protego's AI safety assistant. "
        "I can help you with safety tips and guidance. "
        "How can I help you stay safe today?"
    )
```

#### Location Safety
```python
if test_mode:
    # Use heuristic-based analysis
    # No LLM API call
    safety_score = 85
    if is_late_night:
        safety_score -= 25

    return {
        "safety_score": safety_score,
        "status": "caution",
        "factors": ["Time-based heuristic analysis"],
        "recommendations": ["Stay alert"]
    }
```

### Production Validation

The backend validates that test mode is disabled in production:

```python
def validate_production_config(self) -> None:
    if self.is_production and self.test_mode:
        raise ValueError(
            "CRITICAL: test_mode is enabled in production! "
            "This will prevent real alerts. "
            "Set TEST_MODE=false in .env"
        )

# Called on startup
settings.validate_production_config()
```

---

## Performance Metrics

### AI Service Latency

| Service | Typical Latency | Max Latency |
|---------|----------------|-------------|
| Whisper Transcription | 2-5 seconds | 10 seconds |
| MegaLLM Text Analysis | 1-2 seconds | 5 seconds |
| MegaLLM Chat | 2-3 seconds | 8 seconds |
| MegaLLM Summary | 2-4 seconds | 10 seconds |
| MegaLLM Location | 1-3 seconds | 7 seconds |
| Azure Realtime | 500ms | 2 seconds |

### End-to-End Timing

**One-Time Audio Analysis:**
```
User stops recording â†’ Upload (500ms) â†’ Transcription (3s) â†’
Detection (100ms) â†’ Alert creation (50ms) â†’ UI update (50ms)

Total: ~3.7 seconds
```

**Real-Time Monitoring:**
```
User speaks "help" â†’ VAD detection (200ms) â†’ Transcription (300ms) â†’
AI analysis (500ms) â†’ Alert creation (50ms) â†’ UI update (50ms)

Total: ~1.1 seconds
```

### Cost Optimization

- **Caching:** Redis caches AI results for identical inputs (5-minute TTL)
- **Rate Limiting:** Prevents API abuse and cost spikes
- **Test Mode:** Development without API costs
- **Efficient Prompts:** Minimal token usage with focused prompts

---

## Troubleshooting

### Common Issues

#### 1. Whisper API Returns Empty Response

**Symptom:** Audio analysis returns no transcription

**Causes:**
- Audio file too short (<1 second)
- Unsupported audio format
- API key invalid
- Network timeout

**Solution:**
```python
# Check audio duration
if len(audio_data) < 16000:  # ~1 second at 16kHz
    raise HTTPException(400, "Audio too short")

# Verify API key
if not settings.whisper_api_key:
    logger.error("Whisper API key not configured")
```

#### 2. Azure Realtime WebSocket Connection Fails

**Symptom:** WebSocket connection rejected

**Causes:**
- Invalid API key
- Wrong deployment name
- Endpoint URL incorrect
- CORS issues

**Solution:**
```typescript
// Check config response
const config = await aiAPI.getRealtimeConfig();
console.log('WebSocket URL:', config.ws_url);

// Verify connection
ws.onerror = (error) => {
  console.error('WebSocket error:', error);
  // Check: API key valid? Endpoint correct?
};
```

#### 3. Rate Limit Exceeded

**Symptom:** 429 Too Many Requests

**Causes:**
- Too many requests from same IP
- Development testing without limits

**Solution:**
```python
# Temporarily disable rate limiting for development
# In main.py:
# app.state.limiter = limiter  # Comment out
```

#### 4. False Positive Alerts

**Symptom:** Alerts triggered by normal conversation

**Causes:**
- Confidence threshold too low
- Keywords in normal speech
- Background noise

**Solution:**
```bash
# Increase confidence threshold
ALERT_CONFIDENCE_THRESHOLD=0.85  # Default: 0.8
```

#### 5. Test Mode Enabled in Production

**Symptom:** No real alerts sent

**Causes:**
- TEST_MODE=true in production .env

**Solution:**
```bash
# Check .env file
TEST_MODE=false

# Backend will throw error on startup if enabled in production
```

---

## Summary Table

### AI Services Comparison

| Feature | Whisper | MegaLLM | Azure Realtime (Monitoring) | Azure Realtime (Safety Call) ğŸ†• |
|---------|---------|---------|----------------|----------------|
| **Purpose** | Transcription | Analysis/Chat | Live Monitoring | AI Phone Call |
| **Input** | Audio file | Text | Audio stream | Bidirectional audio |
| **Output** | Text + timestamps | JSON/Natural | JSON + transcript | AI conversation + alerts |
| **Latency** | 2-5 seconds | 1-3 seconds | 500ms | 500ms |
| **Cost** | Low | Medium | High | Very High (~$0.16/5min) |
| **When to Use** | One-time analysis | Text analysis, chat | Continuous monitoring | Deterrent situations |
| **Best For** | Post-recording | Recommendations, summaries | Real-time detection | Fake call with AI friend |

### AI Modes Comparison

| Mode | Technology | Use Case | User Action | Alert Trigger |
|------|-----------|----------|-------------|---------------|
| **One-Time Analysis** | Whisper + Rules | Record short audio for analysis | Manual recording | Keywords (0.8+ confidence) |
| **Real-Time Monitoring** | Azure Realtime | Continuous monitoring during walk | Enable monitoring | AI analysis + keywords |
| **Text Analysis** | MegaLLM | Analyze typed/spoken text | Type or speak text | AI determines emergency |
| **Safety Call** ğŸ†• | Azure Realtime | Fake phone call deterrent | Start safety call | Silent detection (0.75+) |

### Deployment Checklist

Before deploying AI features to production:

- [ ] Set `TEST_MODE=false` in production `.env`
- [ ] Verify all API keys are production keys
- [ ] Set `ALERT_CONFIDENCE_THRESHOLD` appropriately (0.8 recommended)
- [ ] Set `ALERT_COUNTDOWN_SECONDS` appropriately (5 recommended)
- [ ] **ğŸ†• Set `SAFETY_CALL_ENABLED=true` if enabling safety calls**
- [ ] **ğŸ†• Set `SAFETY_CALL_MAX_DURATION_MINUTES` to prevent cost overruns**
- [ ] **ğŸ†• Set `SAFETY_CALL_ALERT_THRESHOLD` (0.75 recommended)**
- [ ] Configure Sentry for error tracking
- [ ] Test audio analysis with real recordings
- [ ] Test real-time monitoring WebSocket connection
- [ ] **ğŸ†• Test safety call end-to-end (start, converse, distress detection, alert)**
- [ ] **ğŸ†• Verify safety call transcripts saved to database**
- [ ] **ğŸ†• Test safety call silent alert triggering (no UI indication)**
- [ ] Verify rate limits are appropriate
- [ ] Test alert recovery after server restart
- [ ] Ensure trusted contacts receive SMS (end-to-end test)

---

## Additional Resources

### Documentation Links

- **Whisper API (Chutes):** https://chutes.ai/docs
- **MegaLLM API:** https://docs.megallm.io
- **Azure OpenAI Realtime:** https://learn.microsoft.com/azure/ai-services/openai/realtime-audio

### Related Protego Documentation

- [COMPLETE_IMPLEMENTATION_REPORT.md](./COMPLETE_IMPLEMENTATION_REPORT.md) - Full implementation details
- [E2E_TEST_GUIDE.md](./E2E_TEST_GUIDE.md) - Testing procedures
- [IMPROVEMENTS_SUMMARY.md](./IMPROVEMENTS_SUMMARY.md) - Security improvements
- [backend/services/ai_service.py](./backend/services/ai_service.py) - AI service source code
- [backend/routers/ai.py](./backend/routers/ai.py) - AI endpoints
- **ğŸ†• [SAFETY_CALL_IMPLEMENTATION.md](./SAFETY_CALL_IMPLEMENTATION.md) - Safety Call technical documentation**
- **ğŸ†• [IMPLEMENTATION_COMPLETE.md](./IMPLEMENTATION_COMPLETE.md) - Safety Call completion summary**
- **ğŸ†• [backend/services/safety_call/](./backend/services/safety_call/) - Safety Call service layer**
- **ğŸ†• [backend/routers/safety_call.py](./backend/routers/safety_call.py) - Safety Call API endpoints**

---

**Document Version:** 2.0
**Last Updated:** 2026-01-16
**Maintained By:** Protego Development Team

**Changelog:**
- **v2.0 (2026-01-16):** Added AI Safety Call feature documentation
- **v1.0 (2026-01-15):** Initial comprehensive AI usage guide
